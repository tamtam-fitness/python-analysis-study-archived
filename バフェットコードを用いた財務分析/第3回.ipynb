{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import *\n",
    "\n",
    "def fetch(ticker,bc_api_endpoint,apikey,FROM,TO):\n",
    "\n",
    "    response = requests.get(\n",
    "                            url=bc_api_endpoint,\n",
    "                            params={\n",
    "                                    \"tickers\":ticker,\n",
    "                                    \"from\":FROM,\n",
    "                                    \"to\":TO,\n",
    "                                    },\n",
    "                                    headers={\n",
    "                                            \"x-api-key\":apikey,\n",
    "                                            },\n",
    "                            )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(ticker_code:str):\n",
    "    ticker = ticker_code.split(\",\")\n",
    "    ####  四半期の財務数値######################################\n",
    "    bc_api_endpoint = \"https://api.buffett-code.com/api/v2/quarter\"\n",
    "    #自分のAPIキーの文字列を挿入\n",
    "    apikey = \"YourApiKey\" \n",
    "\n",
    "    #期間の指定\n",
    "    START_q =\"2019Q1\" #2019年度の第1四半期\n",
    "    END_q =\"2019Q4\"   #2019年度の第4四半期\n",
    "\n",
    "\n",
    "    res_q = fetch(ticker_code,bc_api_endpoint,apikey,START_q,END_q)\n",
    "    json_data_q = json.loads(res_q.text)\n",
    "    #バフェットコードの仕様で一回当たり最大3社が取得可能な為対応\n",
    "    df_q_0 = pd.DataFrame.from_dict(json_data_q[ticker[0]]) #リストの0行目の銘柄\n",
    "    df_q_1 = pd.DataFrame.from_dict(json_data_q[ticker[1]]) #リストの1行目の銘柄\n",
    "    df_q_2 = pd.DataFrame.from_dict(json_data_q[ticker[2]]) #リストの2行目の銘柄\n",
    "    #リスト化をする\n",
    "    df_q_list = [df_q_0,df_q_1,df_q_2]\n",
    "\n",
    "    #古い順でデータをソートする\n",
    "    for i in range(len(df_q_list)):\n",
    "        #to_datetimeに変換\n",
    "        df_q_list[i]['to_datetime'] = pd.to_datetime(df_q_list[i][\"edinet_updated_date\"])\n",
    "        #ソートしたリストを作成\n",
    "        sort_data = sorted(df_q_list[i]['to_datetime'])\n",
    "        #データを再作成\n",
    "        concat_data = pd.DataFrame()\n",
    "        for sort_num in range(len(sort_data)):\n",
    "            #リストを頼りに組み直す\n",
    "            datetime_data = df_q_list[i][df_q_list[i]['to_datetime'] == sort_data[sort_num]]\n",
    "            concat_data = pd.concat([concat_data,datetime_data],axis = 0)\n",
    "        #データを更新\n",
    "        df_q_list[i] = concat_data\n",
    "\n",
    "    ##### 日別の株価指標 #########################################\n",
    "    \n",
    "    #urlの最後の\" / \"の後ろをdailyと指定\n",
    "    bc_api_endpoint = \"https://api.buffett-code.com/api/v2/daily\"\n",
    "\n",
    "    year = 2019 #START_dayの一部をyearに代入\n",
    "    month = 4  #START_dayの一部をmonthに代入\n",
    "    START_day = f\"{year}-0{month}-01\" #2019年度の第1四半期の時系列と合わせる\n",
    "    END_day =  \"2020-03-31\" #2019年度の第4四半期の時系列と合わせる\n",
    "\n",
    "\n",
    "    res_day = fetch(ticker_code, bc_api_endpoint, apikey, START_day, END_day) \n",
    "    json_data_day = json.loads(res_day.text)    \n",
    "\n",
    "    #バフェットコードの仕様で一回当たり最大3社が取得可能な為対応\n",
    "    df_d_0 = pd.DataFrame.from_dict(json_data_day[ticker[0]])\n",
    "    df_d_1 = pd.DataFrame.from_dict(json_data_day[ticker[1]])\n",
    "    df_d_2 = pd.DataFrame.from_dict(json_data_day[ticker[2]])\n",
    "    df_d_list = [df_d_0,df_d_1,df_d_2]\n",
    "    df_d_list[2]\n",
    "\n",
    "\n",
    "    #4分割し平均化したデータをリスト化する\n",
    "    df_day_list = []\n",
    "    for i in range(len(df_d_list)):\n",
    "        #datetimeに変換させ時間の比較を行うようにする\n",
    "        df_d_list[i]['to_datetime'] = pd.to_datetime(df_d_list[i]['day'])\n",
    "\n",
    "        #mergeのための、空フレームを作成\n",
    "        df_merge = pd.DataFrame()\n",
    "        #四半期ごとに分割するために、最初の検索月を指定する際などに利用\n",
    "        searchConditionMonth = month - 1 \n",
    "        #四半期ごとに分割するために、+3か月して値を更新する際などに利用\n",
    "        searchConditionMonthIncrement = 3 \n",
    "        #yearを代入\n",
    "        searchConditionYear = year \n",
    "\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            #4回分割したら終了(1年分）\n",
    "            if 4 + 1 <= count:\n",
    "                break\n",
    "\n",
    "            # 検索条件（From）～年～月１日より期間スタート\n",
    "            searchConditionDatetimeFrom = datetime(searchConditionYear, searchConditionMonth + 1, 1, tzinfo=timezone.utc)\n",
    "\n",
    "            # 検索期間の月と年を更新する\n",
    "            searchConditionMonth = (searchConditionMonth + searchConditionMonthIncrement) % 12\n",
    "            # 12で割ったあまりが1(13月 = 10月 + 3月)になった際、年を更新させる\n",
    "            if searchConditionMonth < searchConditionMonthIncrement:\n",
    "                searchConditionYear += 1\n",
    "\n",
    "            # 検索条件（To）～年～(+3)月 で期間をストップ\n",
    "            searchConditionDatetimeTo = datetime(searchConditionYear, searchConditionMonth + 1, 1, tzinfo=timezone.utc)   \n",
    "\n",
    "            #検索期間内にあるデータを抽出\n",
    "            df_dq = df_d_list[i][(searchConditionDatetimeFrom <= df_d_list[i]['to_datetime']) &\n",
    "                                                                (df_d_list[i]['to_datetime'] < searchConditionDatetimeTo)\n",
    "                                ]\n",
    "            #四半期ごとのデータを平均化しmergeさせる\n",
    "            df_dq_mean = df_dq.mean()\n",
    "            df_DQ = pd.DataFrame(df_dq_mean)\n",
    "            df_merge = pd.concat([df_merge,df_DQ],axis = 1)\n",
    "\n",
    "        #4分割し平均化したデータをリスト化する\n",
    "        df_day_list.append(df_merge.T) #day\n",
    "    \n",
    "    ######  直近営業日の株価指標  ################################################\n",
    "    bc_api_endpoint = \"https://api.buffett-code.com/api/v2/indicator\"\n",
    "    #期間を指定せず、時点(直近営業日)を指定しいているため\"None\"を記入\n",
    "    res_c = fetch(ticker_code,bc_api_endpoint,apikey,None,None)\n",
    "    json_data_c = json.loads(res_c.text)\n",
    "    #バフェットコードの仕様で一回当たり最大3社が取得可能な為対応\n",
    "    df_c_0 = pd.DataFrame.from_dict(json_data_c[ticker[0]]) \n",
    "    df_c_1 = pd.DataFrame.from_dict(json_data_c[ticker[1]]) \n",
    "    df_c_2 = pd.DataFrame.from_dict(json_data_c[ticker[2]]) \n",
    "    df_c_list = [df_c_0,df_c_1,df_c_2]\n",
    "\n",
    "    return df_q_list,df_day_list,df_c_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フォルダを作成\n",
    "# csvデータ保存する\n",
    "ticker_code = input()\n",
    "df_q_list,df_day_list,df_c_list = make_df(ticker_code)\n",
    "df_q_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#上記で書いた3種類のデータ取得の関数をもとに集めたデータを、それぞれcsvへ保存\n",
    "ticker = ticker_code.split(\",\")\n",
    "\n",
    "i = 0\n",
    "for code_num in ticker :\n",
    "    df_q_list[i].to_csv(f\"df_q/{code_num}.csv\")\n",
    "    df_day_list[i].to_csv(f\"df_day/{code_num}.csv\")\n",
    "    df_c_list[i].to_csv(f\"df_c/{code_num}.csv\")\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ticker_list/ticker.txt\" ,mode = \"r\") as f:\n",
    "        file = f.read()\n",
    "        ticker_list_three = file.split(\"\\n\")\n",
    "    \n",
    "ticker_list_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0行で処理が終了しました。\n",
      "0から始まって0まで0行読み込まれました。\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #あらかじめバフェットコード様から抽出できる証券コードのリストを用意(バフェットコード様の業種検索を参考)\n",
    "    with open(\"ticker_list/ticker.txt\" ,mode = \"r\") as f:\n",
    "        file = f.read()\n",
    "    ticker_list_three = file.split(\"\\n\")\n",
    "    #取得終了時にセーブポイントが上書きされる、初期化する(初めから行いたい）場合はカウントを0にする。\n",
    "    \n",
    "    #ticker_listの中に同梱されているticker_count.txtを読み込む(exceptで利用)\n",
    "    with open(\"ticker_list/ticker_count.txt\" , mode = \"r\") as f:\n",
    "        count_int = int(f.read())\n",
    "    count = count_int\n",
    "    memory = count\n",
    "    #カウントを頼りに、ticker_list_threeを上から順番に読み込ませる\n",
    "    for ticker_code in ticker_list_three[count:] :\n",
    "        df_q_list,df_day_list,df_c_list = make_df(ticker_code)\n",
    "        for i in range(3):\n",
    "            ticker_l = ticker_code.split(\",\")\n",
    "            #上記で書いた3種類のデータ取得の関数をもとに集めたデータを、それぞれcsvへ保存\n",
    "            df_q_list[i].to_csv(f\"df_q/{ticker_l[i]}.csv\")\n",
    "            df_day_list[i].to_csv(f\"df_day/{ticker_l[i]}.csv\")\n",
    "            df_c_list[i].to_csv(f\"df_c/{ticker_l[i]}.csv\")\n",
    "        count += 1\n",
    "except :\n",
    "    #リクエスト制限が掛かり一日で取得出来るデータ数が達したらセーブポイントを作成\n",
    "    with open(\"ticker_list/ticker_count.txt\" , mode = \"w\") as f:\n",
    "        f.write(str(count))\n",
    "    print(f\"{count}行で処理が終了しました。\")\n",
    "    print(f\"{memory}から始まって{count}まで{count - memory}行読み込まれました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry =  [\n",
    "             \"水産・農林業\",\n",
    "             \"鉱業\",\n",
    "             \"建設業\",\n",
    "             \"食料品\",\n",
    "             \"繊維製品\",\n",
    "            \"パルプ・紙\",\n",
    "            \"化学\",\n",
    "            \"医薬品\",\n",
    "            \"石油・石炭製品\",\n",
    "            \"ゴム製品\",\n",
    "            \"ガラス・土石製品\",\n",
    "            \"鉄鋼\",\n",
    "            \"非鉄金属\",\n",
    "            \"金属製品\",\n",
    "            \"機械\",\n",
    "            \"電気機器\",\n",
    "            \"輸送用機器\",\n",
    "            \"精密機器\",\n",
    "            \"その他製品\",\n",
    "            \"電気・ガス業\",\n",
    "            \"陸運業\",\n",
    "            \"海運業\",\n",
    "            \"空運業\",\n",
    "            \"倉庫・運輸関連業\",\n",
    "            \"卸売業\",\n",
    "            \"小売業\",\n",
    "            \"銀行業\",\n",
    "            \"証券、商品先物取引業\",\n",
    "            \"保険業\",\n",
    "            \"その他金融業\",\n",
    "            \"不動産業\",\n",
    "            \"サービス業\",\n",
    "            \"情報・通信業\",\n",
    "            \"REIT\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'水産・農林業': 12, '鉱業': 6, '建設業': 158, '食料品': 122, '繊維製品': 54, 'パルプ・紙': 24, '化学': 210, '医薬品': 68, '石油・石炭製品': 12, 'ゴム製品': 20, 'ガラス・土石製品': 56, '鉄鋼': 45, '非鉄金属': 35, '金属製品': 90, '機械': 228, '電気機器': 243, '輸送用機器': 93, '精密機器': 51, 'その他製品': 110, '電気・ガス業': 24, '陸運業': 62, '海運業': 14, '空運業': 6, '倉庫・運輸関連業': 38, '卸売業': 317, '小売業': 348, '銀行業': 83, '証券、商品先物取引業': 41, '保険業': 15, 'その他金融業': 35, '不動産業': 132, 'サービス業': 470, '情報・通信業': 462, 'REIT': 2}\n"
     ]
    }
   ],
   "source": [
    "industry_dict={}\n",
    "for i in industry :\n",
    "    with open(f\"ticker_list/{i}.txt\" ,mode = \"r\") as f:\n",
    "            file = f.read()\n",
    "    ticker_list_three = file.split(\"\\n\")\n",
    "    \n",
    "    industry_list = []\n",
    "    for code in ticker_list_three :\n",
    "        ticker_num = code.split(\",\")\n",
    "        for count in ticker_num :\n",
    "            industry_list.append(count)\n",
    "    \n",
    "    ind_d = {i:len(industry_list)}\n",
    "    industry_dict.update(ind_d)\n",
    "    \n",
    "print(industry_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'建設業': 158,\n",
       " '食料品': 122,\n",
       " '化学': 210,\n",
       " '機械': 228,\n",
       " '電気機器': 243,\n",
       " '卸売業': 317,\n",
       " '小売業': 348,\n",
       " '不動産業': 132,\n",
       " 'サービス業': 470,\n",
       " '情報・通信業': 462}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scr_dict = {}\n",
    "#パラメータ数(指標数)＊10倍以上が機械学習において適正なデータ 今回は12指標なため120以上\n",
    "scr = 12 * 10\n",
    "for idk,idv in industry_dict.items():    \n",
    "    if int(scr) <= idv :\n",
    "        if_dict = {idk:idv}\n",
    "        scr_dict.update(if_dict)\n",
    "        \n",
    "scr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "どの業種を取得しますか？ : サービス業\n",
      "全157行中120行で操作を終了しました。\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    industry_select = input(\"どの業種を取得しますか？ : \")\n",
    "    with open(f\"ticker_list/{industry_select}.txt\" ,mode = \"r\") as f:\n",
    "            file = f.read()\n",
    "    ticker_list_three = file.split(\"\\n\")\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for ticker_code in ticker_list_three :\n",
    "        df_q_list,df_day_list,df_c_list = make_df(ticker_code)\n",
    "        for i in range(3):\n",
    "            ticker_l = ticker_code.split(\",\")\n",
    "            #上記で書いた3種類のデータ取得の関数をもとに集めたデータを、それぞれcsvへ保存\n",
    "            df_q_list[i].to_csv(f\"df_q/{ticker_l[i]}.csv\")\n",
    "            df_day_list[i].to_csv(f\"df_day/{ticker_l[i]}.csv\")\n",
    "            df_c_list[i].to_csv(f\"df_c/{ticker_l[i]}.csv\")\n",
    "        \n",
    "        count += 1\n",
    "except:\n",
    "    print(f\"全{len(ticker_list_three)}行中{count}行で操作を終了しました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
